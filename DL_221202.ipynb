{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17EE-YcxDb5PxMAkYWJC1yrsts_2w0bW1",
      "authorship_tag": "ABX9TyMEnIlUN/x+rNS3jF0Td3i0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bongkyunSON/Deep_Leaning/blob/main/DL_221202.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrgN6PQgEeMl",
        "outputId": "9902fb23-c54f-48d6-ecbf-5866235ae0f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Dec  2 05:34:01 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P0    29W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# os.cpu_count()\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Deep Learning Track. Mini PJT - CIFAR100 classification**\n",
        "\n",
        "\n",
        "저희는 수업시간에 CIFAR10을 분류하는 문제를 풀어보았습니다.\n",
        "\n",
        "이 때 까지 배운 모든 방법을 사용하여, CIFAR100 데이터셋을 100가지로 분류하는 문제를 풀어보세요.\n",
        "프로젝트를 수행할 때, 다음 조건들을 만족하여야 합니다.\n",
        "\n",
        "----\n",
        "\n",
        "1.  MLP, (Vanilla)CNN, ResNet50 중에 2개의 모델을 골라서 구현합니다.\n",
        "\n",
        "\n",
        "2. 각 모델을 design하여 (hyper-parameter를 세팅) 구현합니다.\n",
        "\n",
        "\n",
        "3. Validation Accuracy 기준 0.4를 넘기는 모델과 학습 세팅을 찾습니다.\n",
        "\n",
        "----\n",
        "\n",
        "> 모델을 구현하고 성능을 올리기 위한 팁들을 몇 가지 소개합니다.\n",
        "\n",
        "- learning rate가 optimization에 가장 큰 영향을 끼칩니다. 다양한 숫자를 테스트해보세요.\n",
        "\n",
        "\n",
        "- 처음에 학습이 잘되는 세팅을 찾기 위해서는 10 epoch 정도로 초반 학습 성능이 잘나오는지 확인하는게 중요합니다. (val acc도 포함)\n",
        "\n",
        "\n",
        "- 초반 학습 성능이 잘 나왔다면, epochs를 늘려서 학습이 잘되는지 확인합니다.\n",
        "\n",
        "\n",
        "- epoch를 늘려도 한계에 도달했다면, overfitting이 되었는지 아닌지 확인합니다.\n",
        "\n",
        "\n",
        "- overfitting이 되었다면, 여러 regularization 기법들이 도움이 될 겁니다. e.g. tensorflow.keras.layers.Dropout\n",
        "\n",
        "\n",
        "- batch size도 바꿔보세요. batch size가 full-batch에 가까울 수록, 속도는 느려지지만 optimizer의 방향은 최적의 값을 향합니다.\n",
        "\n",
        "\n",
        "- MLP나 CNN을 구현하고 있다면, 모델의 크기를 바꿔보세요. 파라미터가 많을 수록, 학습할 수 있는 정보가 많아집니다."
      ],
      "metadata": {
        "id": "LvY7EEWDd_eP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "\n",
        "# cifar10과 keras layer들 불러오기\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "from tensorflow.keras.layers import Dense, Input, Flatten, Conv2D, MaxPool2D, Dropout\n",
        "from tensorflow.keras import Sequential"
      ],
      "metadata": {
        "id": "MADLRhqQEkDz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## mini batch 데이터의 변경을 막기 위한 random seed 고정\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "seed_everything(0xC0FFEE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzmVIhXTFD8H",
        "outputId": "39813da4-4313-4f40-aadb-9f90ca245e4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Datasets\n",
        "(X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
        "print(X_train.shape) # 4차원 Tensor\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "JgMdor4WFUaz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        "\n",
        "1. Normalization (v)\n",
        "\n",
        "\n",
        "2. Standardization"
      ],
      "metadata": {
        "id": "H3cY3o09eLgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing = \"standard\" # \"standard\"\n",
        "debug_mode = False"
      ],
      "metadata": {
        "id": "7UQKyvmcFaWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization (Minmax scaling)\n",
        "if preprocessing == \"normalize\":\n",
        "    X_train = X_train.astype(np.float32) / 255.0\n",
        "    X_test = X_test.astype(np.float32) / 255."
      ],
      "metadata": {
        "id": "PKgyI63JePXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standarization\n",
        "if preprocessing == \"standard\":\n",
        "    if debug_mode:\n",
        "        print(\"-- Before --\")\n",
        "        print(\"Training data\\n\", X_train[0])\n",
        "\n",
        "    img_mean = np.mean(X_train, axis=0)\n",
        "    img_std = np.std(X_train, axis=0)\n",
        "\n",
        "    if debug_mode:\n",
        "        print(img_mean.shape)\n",
        "        print(img_std.shape)\n",
        "\n",
        "        print(\"mean : \", np.mean(X_train)) # through data\n",
        "        print(\"std : \", np.std(X_train))\n",
        "\n",
        "    # Scaling\n",
        "    X_train = X_train.astype(np.float32) - img_mean\n",
        "    X_train = X_train.astype(np.float32) / img_std\n",
        "\n",
        "    if debug_mode:\n",
        "        print(\"\\n\\n-- After --\")\n",
        "        #print(\"Training data\\n\", X_train[0])\n",
        "        print(\"mean : \", np.mean(X_train))\n",
        "        print(\"std : \", np.std(X_train))\n",
        "    \n",
        "    # Scaling\n",
        "    X_test = X_test.astype(np.float32) - img_mean\n",
        "    X_test = X_test.astype(np.float32) / img_std"
      ],
      "metadata": {
        "id": "1R9HgWvSF3uk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "id": "4C4MoX-xGToB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training with Vanilla CNN\n",
        "\n",
        "**Model Architecture**\n",
        "\n",
        "> Conv - Relu - Conv - Relu - Pool - Conv - Relu - Conv - Relu - Pool - FC - Softmax"
      ],
      "metadata": {
        "id": "svhCUUOyeYGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 위의 Vanilla CNN 구조를 직접 구현해봅니다.\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(32, 32, 3)),\n",
        "    Conv2D(filters=8, kernel_size=3, strides=1, padding='same', activation='relu'),\n",
        "    Conv2D(filters=16, kernel_size=3, strides=1, padding='same', activation='relu'),\n",
        "    MaxPool2D(pool_size=2),\n",
        "    Conv2D(24, 3, 1, padding='same', activation='relu'),\n",
        "    Conv2D(48, 3, 1, padding='same', activation='relu'),\n",
        "    MaxPool2D(2),\n",
        "    Flatten(),\n",
        "    Dropout(0.2),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(100, activation='softmax') # softmax\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQOdLAKjGV_o",
        "outputId": "9849fd53-b165-47e7-bb7c-36f1aad86a5c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 1s 4ms/step - loss: 2.8908 - accuracy: 0.3376\n",
            "Loss : 2.8908, Accuracy : 0.3376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training setup\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
        "\n",
        "# val_loss 기준 체크포인터도 생성합니다.\n",
        "cnn_filename = \"CNN_best_model.ckpt\"\n",
        "checkpoint = ModelCheckpoint(cnn_filename, \n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1)\n",
        "\n",
        "\n",
        "## config variables\n",
        "batch_size = 50000\n",
        "lr = 1e-3\n",
        "epochs = 30\n",
        "\n",
        "# learning rate를 업데이트 해보려고 합니다.\n",
        "def scheduler(epoch, learning_rate):\n",
        "    if epoch < 25:\n",
        "        return learning_rate\n",
        "    elif epoch < 50:\n",
        "        return lr/10\n",
        "    elif epoch < 75:\n",
        "        return lr/50\n",
        "    else:\n",
        "        return lr/100    \n",
        "\n",
        "# Set optimizer, loss function, metrics, callback function\n",
        "#optimizer = SGD(learning_rate=lr, momentum=0.9) # Momentum optimizer \n",
        "optimizer = Adam(learning_rate=lr)\n",
        "loss_fn = 'sparse_categorical_crossentropy' # multi-class classification\n",
        "metrics = ['accuracy']\n",
        "lr_callback = LearningRateScheduler(scheduler)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss_fn,\n",
        "              metrics=metrics)"
      ],
      "metadata": {
        "id": "0_U2jBL1IwGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "# 한번만 직접 해봅시다!\n",
        "\n",
        "history = model.fit(x=X_train,\n",
        "                    y=y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    callbacks=[lr_callback, checkpoint],\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "cZee6FkeKnCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate\n",
        "loss, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "print(\"Loss : %.4f, Accuracy : %.4f\" % (loss, acc))"
      ],
      "metadata": {
        "id": "NGEO7WCCeiH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## loss visualize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(1, 2,1)\n",
        "plt.plot(history.history['loss'],'b-', label = \"training\")\n",
        "plt.plot(history.history['val_loss'], 'r:', label = \"validation\")\n",
        "plt.title(\"model - loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"model - accuracy\")\n",
        "\n",
        "plt.plot(history.history['accuracy'], 'b-', label = \"training\")\n",
        "plt.plot(history.history['val_accuracy'], 'r:', label = \"validation\")\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZXnQwwcBejoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 이번엔 ResNet50를 사용해봅시다.\n",
        "\n",
        "- ResNet50을 사용하기 위해서는 keras에 구현되어 있는 ResNet50 구조를 가져옵니다.\n",
        "\n",
        "- pretrained를 가져오는 경우도 있지만, 지금은 ImageNet 세팅이 아니므로 구조만 가져옵니다.(input이 완전히 다름)\n",
        "\n",
        "- Reference : https://keras.io/api/applications/resnet/#resnet50-function"
      ],
      "metadata": {
        "id": "ADkMlrU_elbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load ResNet50\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet import ResNet152\n",
        "\n",
        "resnet = ResNet50(\n",
        "    include_top=True,        # classification : True,  embedding : False\n",
        "    weights=None,             # ImageNet pretrained : True, else : None\n",
        "    input_shape=(32, 32, 3),  # if not pretrained, you must specify.\n",
        "    pooling='max',\n",
        "    classes=100,               # number of classes\n",
        "    classifier_activation='softmax'    # output function\n",
        ")"
      ],
      "metadata": {
        "id": "LuOodII_enLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see ResNet50 model\n",
        "#resnet.summary()"
      ],
      "metadata": {
        "id": "KnGHLiIKeoZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### resnet training strategy\n",
        "# val_loss 기준 체크포인터도 생성합니다.\n",
        "resnet_filename = \"resnet_best_model.ckpt\"\n",
        "checkpoint = ModelCheckpoint(resnet_filename, \n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1)\n",
        "\n",
        "\n",
        "## config variables\n",
        "batch_size = 64\n",
        "lr = 5e-3\n",
        "epochs = 30\n",
        "\n",
        "# learning rate를 업데이트 해보려고 합니다\n",
        "def scheduler(epoch, learning_rate):\n",
        "    if epoch < 5:\n",
        "        return learning_rate\n",
        "    elif epoch < 10:\n",
        "        return lr/10\n",
        "    elif epoch < 15:\n",
        "        return lr/50\n",
        "    else:\n",
        "        return lr/100\n",
        "    \n",
        "#optimizer = SGD(learning_rate=lr, momentum=0.9)\n",
        "optimizer = Adam(learning_rate=lr)\n",
        "loss_fn = 'sparse_categorical_crossentropy'\n",
        "metrics = ['accuracy']\n",
        "\n",
        "resnet.compile(optimizer=optimizer,\n",
        "               loss=loss_fn,\n",
        "               metrics=metrics)"
      ],
      "metadata": {
        "id": "Nbz5jEUCepak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training ResNet50\n",
        "resnet_history = resnet.fit(X_train, y_train,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            validation_data=(X_test, y_test),\n",
        "                            callbacks=[lr_callback, checkpoint],\n",
        "                            verbose=1)"
      ],
      "metadata": {
        "id": "hTnwLZw2eq6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate\n",
        "loss, acc = resnet.evaluate(X_test, y_test, batch_size=batch_size)\n",
        "print(\"Loss : %.4f, Accuracy : %.4f\" % (loss, acc))"
      ],
      "metadata": {
        "id": "3a1Ye36-es9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## loss visualize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.subplot(1, 2,1)\n",
        "plt.plot(resnet_history.history['loss'],'b-', label = \"training\")\n",
        "plt.plot(resnet_history.history['val_loss'], 'r:', label = \"validation\")\n",
        "plt.title(\"model - loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"model - accuracy\")\n",
        "\n",
        "plt.plot(resnet_history.history['accuracy'], 'b-', label = \"training\")\n",
        "plt.plot(resnet_history.history['val_accuracy'], 'r:', label = \"validation\")\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5Wqk5thzeuHF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}